\chapter{Related Works} \label{chap:related_works}

In this section we are going to review some of the most relevant works in the literature that address the optimization of QNN bit-widths while also providing formal guarantees on the network's behavior.

\section {Certified Quantization Strategy Synthesis for Neural Networks}
The work introduces Quadapter, a end-to-end framework for quantizing neural networks by choosing per-layer fixed-point precisions that provably preserve a desired property of robustnes and backdoor-freeness. Within the work, NN are written as a composition of affine and activation layers \(N=f_{2d}\circ\cdots\circ f_1\). For every even affine layer \(2i\), the authors compute a safe preimage \(\mathcal P^{2i}\) such that any activation \(x_{2i}\in\mathcal P^{2i}\) is guaranteed to map \cite{henzinger2020preimage}, through the suffix subnetwork \(N_{[2i+1:2d]}\), into the target output set \(O\), i.e, we garantee that for a given input interval, the output will remain in the desired output set. Preimages are built by solving a layer-wise MILP that maximizes the size of a box-like template \(T^{2i}\) under the inclusion constraint above; inclusion is checked by negating it and asking the MILP for a violating property. Given these backward preimages, the method then selects, for each affine layer, a bit-width configuration \((Q_i,F_i)\) so that a sound forward over-approximation of the quantized activations  remains inside \(\mathcal P^{2i}\); if the MILP-based negated inclusion is unsatisfiable at every certified layer, the whole quantized network is certified for the property. The approach yields compact bit-width vectors on MNIST-like models while maintaining the specified property, and it cleanly separates (i) tight but costlier under-approximations of backward preimages MILP from (ii) fast over-approximations of forward reach sets for inclusion checks. Limitations include reliance on MILP scalability and the need to align forward abstractions with hardware semantics.

The work lacks bit-accurate guarantee for the quantized network, as the forward abstraction does not model rounding and saturation like \cite{baranowski2020smt}. Therefore, w good contribution would be to extend the method to use SMT-based abstraction to check the fixed-point constrains of rounding/saturation.
%the authors note that hybridizations (e.g., using SMT with bit-vectors as a final auditor for rounding/saturation) are a natural extension to improve bit-accurate guarantees without sacrificing the synthesis efficiency.

\section {Counter Example Guided Search for Neural Network Quantization}
Based on the works of \cite{katz2017reluplex,katz2019marabou} the authors propose CEG4N, an end-to-end framework for quantization of neural networks with formal guarantees of accuracy. The framework is divided into 3 main steps: (i) Bit Search Module (BSM) based on genetic algorithm that receives counter-example ($H^{+1}_{CE}$) from the verifier (iii); (ii) Translater that converts both NN and QNN to ONNX format to the input format of the verifier; (iii) Verifier that checks if the QNN is equivalent to the NN within a given tolerance $\epsilon$. The verifier is based on an SMT solver (ESBMC) that encodes the equivalence property as a set of constraints   
$\phi := \phi \land \phi' \land \neg(y \approx_\epsilon y')$ where $\phi$ and $\phi'$ encode the NN and QNN respectively, and $y \approx_\epsilon y'$ encodes the tolerance condition. The verifier returns a counter-example $H^{+1}_{CE}$ if the property is violated, which is then used by the BSM to generate a new bit-width configuration. The process is repeated until either a valid configuration is found or a maximum number of iterations is reached. The authors evaluate their framework on several benchmarks, including MNIST and CIFAR-10, demonstrating its effectiveness in finding low-bit configurations while maintaining accuracy within the specified tolerance.

The main limitation of the work is the scalability of the verifier, which can become a bottleneck for larger networks. For future works, the authors suggest exploring quantization approachs that operate entirelly in integer domain, pontentially improving the scalability. 

 